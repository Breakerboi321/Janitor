from flask import Flask, request, jsonify, Response
import requests
import json
import time
from datetime import datetime

app = Flask(__name__)

# Configuration
NVIDIA_NIM_BASE_URL = "https://integrate.api.nvidia.com/v1"  # Default NIM endpoint
NVIDIA_API_KEY = "your-nvidia-api-key-here"  # Replace with your NVIDIA API key

# Model mapping (OpenAI model names to NVIDIA NIM models)
MODEL_MAPPING = {
    "gpt-3.5-turbo": "meta/llama-3.1-8b-instruct",
    "gpt-4": "meta/llama-3.1-70b-instruct",
    "gpt-4-turbo": "meta/llama-3.1-405b-instruct",
}

def get_nim_model(openai_model):
    """Convert OpenAI model name to NIM model name"""
    return MODEL_MAPPING.get(openai_model, "meta/llama-3.1-8b-instruct")

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    try:
        data = request.json
        
        # Extract OpenAI format parameters
        messages = data.get('messages', [])
        model = data.get('model', 'gpt-3.5-turbo')
        temperature = data.get('temperature', 0.7)
        max_tokens = data.get('max_tokens', 1024)
        stream = data.get('stream', False)
        top_p = data.get('top_p', 1.0)
        
        # Convert to NIM format
        nim_model = get_nim_model(model)
        
        nim_payload = {
            "model": nim_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stream": stream
        }
        
        # Make request to NVIDIA NIM
        headers = {
            "Authorization": f"Bearer {NVIDIA_API_KEY}",
            "Content-Type": "application/json"
        }
        
        if stream:
            return handle_streaming_response(nim_payload, headers, model)
        else:
            return handle_regular_response(nim_payload, headers, model)
            
    except Exception as e:
        return jsonify({
            "error": {
                "message": str(e),
                "type": "proxy_error",
                "code": "internal_error"
            }
        }), 500

def handle_regular_response(nim_payload, headers, original_model):
    """Handle non-streaming responses"""
    response = requests.post(
        f"{NVIDIA_NIM_BASE_URL}/chat/completions",
        headers=headers,
        json=nim_payload,
        timeout=120
    )
    
    if response.status_code != 200:
        return jsonify({
            "error": {
                "message": response.text,
                "type": "nim_error",
                "code": response.status_code
            }
        }), response.status_code
    
    nim_response = response.json()
    
    # Convert NIM response to OpenAI format
    openai_response = {
        "id": nim_response.get("id", f"chatcmpl-{int(time.time())}"),
        "object": "chat.completion",
        "created": int(time.time()),
        "model": original_model,
        "choices": nim_response.get("choices", []),
        "usage": nim_response.get("usage", {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        })
    }
    
    return jsonify(openai_response)

def handle_streaming_response(nim_payload, headers, original_model):
    """Handle streaming responses"""
    def generate():
        try:
            response = requests.post(
                f"{NVIDIA_NIM_BASE_URL}/chat/completions",
                headers=headers,
                json=nim_payload,
                stream=True,
                timeout=120
            )
            
            for line in response.iter_lines():
                if line:
                    line_str = line.decode('utf-8')
                    if line_str.startswith('data: '):
                        data_str = line_str[6:]
                        if data_str.strip() == '[DONE]':
                            yield f"data: [DONE]\n\n"
                            break
                        
                        try:
                            nim_chunk = json.loads(data_str)
                            # Convert to OpenAI format
                            openai_chunk = {
                                "id": nim_chunk.get("id", f"chatcmpl-{int(time.time())}"),
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": original_model,
                                "choices": nim_chunk.get("choices", [])
                            }
                            yield f"data: {json.dumps(openai_chunk)}\n\n"
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            error_chunk = {
                "error": {
                    "message": str(e),
                    "type": "proxy_error"
                }
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/v1/models', methods=['GET'])
def list_models():
    """List available models in OpenAI format"""
    models = []
    for openai_model, nim_model in MODEL_MAPPING.items():
        models.append({
            "id": openai_model,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "nvidia-nim",
            "permission": [],
            "root": openai_model,
            "parent": None
        })
    
    return jsonify({
        "object": "list",
        "data": models
    })

@app.route('/v1/models/<model_id>', methods=['GET'])
def get_model(model_id):
    """Get specific model details"""
    if model_id in MODEL_MAPPING:
        return jsonify({
            "id": model_id,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "nvidia-nim",
            "permission": [],
            "root": model_id,
            "parent": None
        })
    else:
        return jsonify({
            "error": {
                "message": f"Model {model_id} not found",
                "type": "invalid_request_error",
                "code": "model_not_found"
            }
        }), 404

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

if __name__ == '__main__':
    print("=" * 60)
    print("OpenAI-Compatible NVIDIA NIM Proxy Server")
    print("=" * 60)
    print(f"Server starting on http://0.0.0.0:5000")
    print(f"\nIMPORTANT: Update NVIDIA_API_KEY in the code!")
    print(f"\nEndpoints:")
    print(f"  - POST /v1/chat/completions")
    print(f"  - GET  /v1/models")
    print(f"  - GET  /health")
    print(f"\nFor Janitor AI, use: http://YOUR_SERVER_IP:5000")
    print("=" * 60)
    
    # Run on all interfaces so it's accessible from Android
    app.run(host='0.0.0.0', port=5000, debug=False)
